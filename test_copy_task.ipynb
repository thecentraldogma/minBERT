{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77e5123-288f-4948-8ad6-a19de2fa1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699d4c76-8bb5-4116-a156-1d40ea21b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_copy_task(\n",
    "    block_ctor,              # your TransformerBlock class\n",
    "    n_steps=400,\n",
    "    device=None,\n",
    "    B=64, T=16,\n",
    "    vocab_size=32,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    d_ff=256,\n",
    "    n_layers=2,\n",
    "    lr=3e-4,\n",
    "):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # data\n",
    "    train_ds = CopyShiftDataset(num_samples=5000, T=T, vocab_size=vocab_size, seed=123)\n",
    "    val_ds   = CopyShiftDataset(num_samples=500,  T=T, vocab_size=vocab_size, seed=999)\n",
    "    train_dl = DataLoader(train_ds, batch_size=B, shuffle=True, drop_last=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=B, shuffle=False, drop_last=False)\n",
    "\n",
    "    # model\n",
    "    model = TinyTransformerLM(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        d_ff=d_ff,\n",
    "        n_layers=n_layers,\n",
    "        seq_len=T,\n",
    "        emb_p_drop=0.0,\n",
    "        ff_p_drop=0.1,\n",
    "        block_ctor=block_ctor,\n",
    "    ).to(device)\n",
    "\n",
    "    # opt & loss\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()  # expects (B*T, vocab) vs (B*T,)\n",
    "\n",
    "    def evaluate(dloader):\n",
    "        model.eval()\n",
    "        tot, cnt = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inp, tgt in dloader:\n",
    "                inp, tgt = inp.to(device), tgt.to(device)         # (B,T)\n",
    "                logits = model(inp)                               # (B,T,V)\n",
    "                loss = loss_fn(logits.reshape(-1, vocab_size), tgt.reshape(-1))\n",
    "                tot += loss.item() * inp.size(0)\n",
    "                cnt += inp.size(0)\n",
    "        model.train()\n",
    "        return tot / cnt\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    step = 0\n",
    "    it = iter(train_dl)\n",
    "    best_val = float(\"inf\")\n",
    "    while step < n_steps:\n",
    "        try:\n",
    "            inp, tgt = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(train_dl)\n",
    "            inp, tgt = next(it)\n",
    "\n",
    "        inp, tgt = inp.to(device), tgt.to(device)                 # (B,T)\n",
    "        logits = model(inp)                                       # (B,T,V)\n",
    "        loss = loss_fn(logits.reshape(-1, vocab_size), tgt.reshape(-1))\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if (step % 50) == 0:\n",
    "            val_loss = evaluate(val_dl)\n",
    "            best_val = min(best_val, val_loss)\n",
    "            print(f\"step {step:4d} | train_loss {loss.item():.4f} | val_loss {val_loss:.4f}\")\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    # quick qualitative check on a small batch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inp, tgt = next(iter(val_dl))\n",
    "        inp = inp.to(device)\n",
    "        logits = model(inp)                                       # (B,T,V)\n",
    "        pred = logits.argmax(dim=-1).cpu()                        # (B,T)\n",
    "        print(\"\\nSample predictions (first 3 rows):\")\n",
    "        for i in range(min(3, inp.size(0))):\n",
    "            print(\"inp :\", inp[i].tolist())\n",
    "            print(\"tgt :\", ([0] + inp[i].tolist()[:-1]))\n",
    "            print(\"pred:\", pred[i].tolist())\n",
    "            print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b48db-402f-46a0-80fb-1111a8fece04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ac33a4-f69c-4d38-98e7-165dd14e3be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0 | train_loss 2.1747 | val_loss 2.1679\n",
      "step   50 | train_loss 1.9505 | val_loss 1.9531\n",
      "step  100 | train_loss 1.8761 | val_loss 1.8714\n",
      "step  150 | train_loss 1.8072 | val_loss 1.7909\n",
      "step  200 | train_loss 1.7450 | val_loss 1.7325\n",
      "step  250 | train_loss 1.7033 | val_loss 1.6919\n",
      "step  300 | train_loss 1.6990 | val_loss 1.6827\n",
      "step  350 | train_loss 1.6959 | val_loss 1.6798\n",
      "step  400 | train_loss 1.6941 | val_loss 1.6760\n",
      "step  450 | train_loss 1.6888 | val_loss 1.6717\n",
      "step  500 | train_loss 1.6787 | val_loss 1.6697\n",
      "step  550 | train_loss 1.6825 | val_loss 1.6681\n",
      "step  600 | train_loss 1.6797 | val_loss 1.6641\n",
      "step  650 | train_loss 1.6587 | val_loss 1.6602\n",
      "step  700 | train_loss 1.6755 | val_loss 1.6571\n",
      "step  750 | train_loss 1.6674 | val_loss 1.6545\n",
      "step  800 | train_loss 1.6499 | val_loss 1.6542\n",
      "step  850 | train_loss 1.6582 | val_loss 1.6510\n",
      "step  900 | train_loss 1.6633 | val_loss 1.6470\n",
      "step  950 | train_loss 1.6543 | val_loss 1.6470\n",
      "\n",
      "Sample predictions (first 3 rows):\n",
      "inp : [4, 4, 3, 1, 2, 4, 2, 1, 5, 1, 1, 3, 6, 1, 2, 4]\n",
      "tgt : [0, 4, 4, 3, 1, 2, 4, 2, 1, 5, 1, 1, 3, 6, 1, 2]\n",
      "pred: [0, 1, 5, 2, 5, 5, 1, 2, 5, 2, 2, 5, 5, 2, 1, 2]\n",
      "---\n",
      "inp : [5, 5, 2, 6, 2, 2, 2, 4, 2, 1, 2, 5, 3, 6, 2, 3]\n",
      "tgt : [0, 5, 5, 2, 6, 2, 2, 2, 4, 2, 1, 2, 5, 3, 6, 2]\n",
      "pred: [0, 2, 2, 2, 5, 2, 2, 5, 5, 2, 2, 2, 5, 2, 1, 2]\n",
      "---\n",
      "inp : [5, 2, 1, 5, 4, 4, 3, 5, 5, 5, 1, 5, 3, 3, 2, 5]\n",
      "tgt : [0, 5, 2, 1, 5, 4, 4, 3, 5, 5, 5, 1, 5, 3, 3, 2]\n",
      "pred: [0, 2, 2, 2, 5, 2, 5, 2, 2, 2, 2, 2, 5, 5, 2, 2]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "run_copy_task(\n",
    "    TransformerBlock,              # your TransformerBlock class\n",
    "    n_steps=1000,\n",
    "    device=None,\n",
    "    B=64, T=16,\n",
    "    vocab_size=7,\n",
    "    d_model=10,\n",
    "    n_heads=2,\n",
    "    d_ff=128,\n",
    "    n_layers=3,\n",
    "    lr=3e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca96019-3bc2-45f6-8355-a871e5033ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f952658-ba2b-4e7d-8ab1-26e5cc93b64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cd9a986-e9ca-43cb-8f2e-e6ff22326b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011707544326782227,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 42,
       "postfix": null,
       "prefix": "tokenizer_config.json",
       "rate": null,
       "total": 48,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353ae8f3442946999381e90a8268069d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006024837493896484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 42,
       "postfix": null,
       "prefix": "vocab.txt",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c12576fdcb4158b0df0fb92a83c17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019786357879638672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 42,
       "postfix": null,
       "prefix": "tokenizer.json",
       "rate": null,
       "total": 466062,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a95f5a6ef3a4386a1cfe70c6492e9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019728660583496094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 42,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 570,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e73836c8544bfcaadcd094cc267372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "pad_id   = tok.pad_token_id        # usually 0\n",
    "mask_id  = tok.mask_token_id       # 103\n",
    "cls_id   = tok.cls_token_id        # 101\n",
    "sep_id   = tok.sep_token_id        # 102\n",
    "vocab_sz = tok.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "865b7d52-c9c5-435a-a1da-715feaaa5cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64258e13-1ee4-4560-b686-e16c3cd28c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26a8a7d3-69af-4530-b563-0a8171b3e499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db3fba6e-4e26-4950-b615-d707073b0e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "520b0d5d-5879-45ab-bce8-cc362f5771db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bfde977-8592-4ba3-b2ae-7ba096121ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85ae9547-aaef-4b98-8c97-20d2b37de501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class bert_test_dataset(Dataset):\n",
    "    # this is a Dataset class that generates token id sequences\n",
    "    def __init__(self, seq_len, vocab_size, num_samples): \n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return a tensor of length T that starts at a random integer between 0 and vocab_size-1 inclusize and wraps around if needed\n",
    "        import random\n",
    "        i = random.randrange(self.vocab_size)\n",
    "        x = list(range(i, min(i + self.seq_len, self.vocab_size)))  + list(range(0, max(0, i + self.seq_len - self.vocab_size)))\n",
    "        return torch.tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b08fc89-2028-416e-bd3f-716d1f056d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "btd = bert_test_dataset(5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e68dbb7-f963-4fbc-9224-be8ec5491aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "575227db-3b5e-4707-82a5-6a6cf481c0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = btd[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb9d0810-d106-4bcf-a69a-2b0622ab2e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([6, 7, 8, 9, 0]),\n",
       " tensor([0, 1, 2, 3, 4]),\n",
       " tensor([2, 3, 4, 5, 6]),\n",
       " tensor([9, 0, 1, 2, 3]),\n",
       " tensor([4, 5, 6, 7, 8])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ed700-a973-4c49-b0f1-c340b8c5e9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
